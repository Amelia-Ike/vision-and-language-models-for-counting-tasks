# -*- coding: utf-8 -*-
"""Visual7W_GPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/169Ia1FBHWNTtQilLK_PbIAuxBr-EIRgz
"""

!pip install datasets transformers sentencepiece -qqq

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
from PIL import Image
import io
import pandas as pd
import numpy as np
import requests
from sklearn.metrics import classification_report

from transformers import CLIPProcessor, CLIPModel
from transformers import AlignProcessor, AlignModel
from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
from transformers import AltCLIPModel, AltCLIPProcessor
from transformers import ChineseCLIPProcessor, ChineseCLIPModel
from transformers import AutoProcessor, Pix2StructForConditionalGeneration
from transformers import ViltProcessor, ViltForMaskedLM

"""

Dataset: [Visual7W-GPT](https://huggingface.co/datasets/ruanchaves/visual7w-gpt)"""

class ModelEvaluation(object):

  def __init__(self, model, processor, dataset, device):
    self.model = model
    self.processor = processor
    self.dataset = dataset
    self.device = device
    self.model.to(device)

    self.df = self.image_dataset_to_df()

  def image_dataset_to_df(self):
    df = self.dataset.to_pandas()
    df = df.groupby(by=['image_id']).agg(list)
    df['image'] = df['image'].apply(lambda x: x[0]['bytes'])
    return df

  def process_image(self, image_bytes):
      image = Image.open(io.BytesIO(image_bytes))
      return image

  def get_outputs_from_inputs(self, inputs, image):
    model_inputs = self.processor(text=inputs, images=image, return_tensors="pt", padding=True)
    model_inputs.to(self.device)
    outputs = self.model(**model_inputs)
    return outputs

  def get_probs_from_outputs(self, outputs):
    # Calculate the image-text similarity score and softmax to get probabilities
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)
    predicted_probabilities = probs.detach().cpu().numpy()
    return predicted_probabilities

  def process_data(self, row):
    declarative = row['declarative']
    entailment = row['entailment']
    image = self.process_image(row['image'])

    declarative_outputs = self.get_outputs_from_inputs(declarative, image)
    declarative_probs = self.get_probs_from_outputs(declarative_outputs)

    entailment_outputs = self.get_outputs_from_inputs(entailment, image)
    entailment_probs = self.get_probs_from_outputs(entailment_outputs)

    return list(declarative_probs)[0], list(entailment_probs)[0]

  def get_evaluation_data(self, row, alpha=0.5, beta=0.5):
    declarative_probs = np.array(row['declarative_probs'])
    entailment_probs = np.array(row['entailment_probs'])

    probs = alpha * declarative_probs + beta * entailment_probs

    prediction_index = np.argmax(probs)
    prediction = row['answer'][prediction_index]

    if 'True' in row['label']:
      gold = row['answer'][row['label'].index('True')]
    else:
      gold = prediction

    return gold, prediction

  def get_accuracy_matrix(self, max_value=2.1):
    beta_values = np.arange(0, max_value, 0.1)
    accuracy_matrix = np.zeros((len(beta_values)))
    for j, beta in enumerate(beta_values):
        correct_predictions = 0
        for index, row in self.df.iterrows():
            prediction, gold = self.get_evaluation_data(row, alpha=1, beta=beta)
            if prediction == gold:
                correct_predictions += 1
        accuracy_matrix[j] = correct_predictions / len(self.df)
    return beta_values, accuracy_matrix

  def plot_accuracy_matrix(self, beta_values, accuracy_matrix):
    plt.plot(beta_values, accuracy_matrix)
    plt.grid(True)

    # Set the x-axis labels
    # plt.xticks(np.arange(0, 1.1, 0.1), [f'{x:.1f}' for x in np.arange(0, 1.1, 0.1)])

    # Set the y-axis labels
    # plt.yticks(np.arange(0, 1.1, 0.1), [f'{y:.1f}' for y in np.arange(0, 1.1, 0.1)])

    plt.title("Accuracy Plot")
    plt.xlabel("Beta Values")
    plt.ylabel("Accuracy")
    plt.show()

  def evaluate(self, max_value=5):
    self.df[['declarative_probs', 'entailment_probs']] = self.df.apply(self.process_data, axis=1, result_type='expand')
    beta_values, accuracy_matrix = self.get_accuracy_matrix(max_value=max_value)
    return beta_values, accuracy_matrix

  def evaluate_best(self, best=True, beta=None, max_value=5):
    if not ( 'declarative_probs' in self.df.columns and 'entailment_probs' in self.df.columns ):
      beta_values, accuracy_matrix = self.evaluate()
    else:
      beta_values, accuracy_matrix = self.get_accuracy_matrix(max_value=max_value)
    if best:
      best_beta = beta_values[np.argmax(accuracy_matrix)]
    else:
      best_beta = beta
    y_true = []
    y_pred = []
    for index, row in self.df.iterrows():
      gold, prediction = self.get_evaluation_data(row, alpha=1, beta=best_beta)
      y_true.append(gold)
      y_pred.append(prediction)
    return classification_report(y_true, y_pred, output_dict=True)

"""# CLIP"""

dataset = load_dataset("ruanchaves/visual7w-gpt", split='train')
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
device = 'cuda'

evaluator = ModelEvaluation(model, processor, dataset, device)
beta_values, accuracy_matrix = evaluator.evaluate()
evaluator.plot_accuracy_matrix(beta_values, accuracy_matrix)

clip_report = evaluator.evaluate_best()
pd.DataFrame(clip_report).transpose()

clip_report = evaluator.evaluate_best(best=False, beta=0.0)
pd.DataFrame(clip_report).transpose()

"""# ALIGN"""

dataset = load_dataset("ruanchaves/visual7w-gpt", split='train')
model = AlignModel.from_pretrained("kakaobrain/align-base")
processor = AlignProcessor.from_pretrained("kakaobrain/align-base")
device = 'cuda'

evaluator = ModelEvaluation(model, processor, dataset, device)
beta_values, accuracy_matrix = evaluator.evaluate()
evaluator.plot_accuracy_matrix(beta_values, accuracy_matrix)

align_report = evaluator.evaluate_best()
pd.DataFrame(align_report).transpose()

align_report = evaluator.evaluate_best(best=False, beta=0.0)
pd.DataFrame(align_report).transpose()

"""# ALTCLIP"""

dataset = load_dataset("ruanchaves/visual7w-gpt", split='train')
model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
processor = AltCLIPProcessor.from_pretrained("BAAI/AltCLIP")
device = 'cuda'

evaluator = ModelEvaluation(model, processor, dataset, device)
beta_values, accuracy_matrix = evaluator.evaluate()
evaluator.plot_accuracy_matrix(beta_values, accuracy_matrix)

altclip_report = evaluator.evaluate_best()
pd.DataFrame(altclip_report).transpose()

altclip_report = evaluator.evaluate_best(best=False, beta=0.0)
pd.DataFrame(altclip_report).transpose()

"""# ViLT

https://huggingface.co/docs/transformers/model_doc/vilt
"""

from transformers import ViltProcessor, ViltForMaskedLM
import requests
from PIL import Image
import re
import torch
import copy

url = "http://images.cocodataset.org/val2017/000000039769.jpg"

image = Image.open(requests.get(url, stream=True).raw)

text = "a bunch of [MASK] laying on a couch."

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")

model = ViltForMaskedLM.from_pretrained("dandelin/vilt-b32-mlm")

# prepare inputs

encoding = processor(image, text, return_tensors="pt")

# forward pass

outputs = model(**encoding)
inferred_token = [text]

# gradually fill in the MASK tokens, one by one

mask_outputs = []
with torch.no_grad():
  encoded = processor.tokenizer(inferred_token)

  input_ids = torch.tensor(encoded.input_ids)

  encoded = encoded["input_ids"][0][1:-1]

  outputs = model(input_ids=input_ids, pixel_values=encoding.pixel_values)

  mlm_logits = outputs.logits[0]  # shape (seq_len, vocab_size)

  # only take into account text features (minus CLS and SEP token)

  mlm_logits = mlm_logits[1 : input_ids.shape[1] - 1, :]

  # Calculate softmax once outside the loop
  mlm_softmax = mlm_logits.softmax(dim=-1).topk(mlm_logits.shape[1], dim=-1)

  mlm_topk_values, mlm_topk_ids = mlm_softmax
  for i in range(0, mlm_logits.shape[1]):
    mlm_values = mlm_topk_values[:, i]
    mlm_ids = mlm_topk_ids[:, i]
    mlm_values[torch.tensor(encoded) != 103] = 0
    select = mlm_values.argmax().item()
    select_id = mlm_ids[select].item()
    output = processor.decode([select_id])
    mask_outputs.append(output)

dataset = load_dataset("ruanchaves/visual7w-gpt", split='train')

def image_dataset_to_df(dataset):
    df = dataset.to_pandas()
    df = df[df['label']=='True']
    return df

data = image_dataset_to_df(dataset)
data

data.head(10)

# Assuming your data is in a DataFrame called 'df'
data['declarative'] = data['declarative'].str.replace('\d+', "[MASK]", regex=True)

# Print the modified DataFrame
data

from transformers import ViltProcessor, ViltForMaskedLM
import requests
from PIL import Image
import re
import torch
import copy
from io import BytesIO

# Prepare inputs for each image
encoded_images = []
for _, row in data.iterrows():
    # Get the image bytes for the current row
    image_bytes = row['image']['bytes']

    # Create a BytesIO object from the image bytes
    image_file = BytesIO(image_bytes)

    # Open the image using PIL
    image = Image.open(image_file)

    encoded_images.append(image)

text = data['declarative'].values.tolist()

assert len(text) == len(encoded_images)

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
model = ViltForMaskedLM.from_pretrained("dandelin/vilt-b32-mlm")

all_ranks = []

for text, image in zip(text, encoded_images):
  encoding = processor(images=image, text=text, return_tensors="pt")

  outputs = model(**encoding)
  inferred_token = [text]

# gradually fill in the MASK tokens, one by one

  mask_outputs = []
  with torch.no_grad():

      encoded = processor.tokenizer(inferred_token, truncation=True, padding=True)

      input_ids = torch.tensor(encoded.input_ids)

      encoded = encoded["input_ids"][0][1:-1]

      outputs = model(input_ids=input_ids, pixel_values=encoding.pixel_values)

      mlm_logits = outputs.logits[0]  # shape (seq_len, vocab_size)

      # only take into account text features (minus CLS and SEP token)

      mlm_logits = mlm_logits[1:input_ids.shape[1] - 1, :]

      # Calculate softmax once outside the loop
      mlm_softmax = mlm_logits.softmax(dim=-1).topk(mlm_logits.shape[1], dim=-1)

      mlm_topk_values, mlm_topk_ids = mlm_softmax
      for i in range(0, mlm_logits.shape[1]):
          mlm_values = mlm_topk_values[:, i]
          mlm_ids = mlm_topk_ids[:, i]
          mlm_values[torch.tensor(encoded) != 103] = 0
          select = mlm_values.argmax().item()
          select_id = mlm_ids[select].item()
          output = processor.decode([select_id])
          mask_outputs.append(output)

  all_ranks.append(mask_outputs)

def is_number_between_zero_and_twenty(num_str):
    number_words = {
        'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6,
        'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10, 'eleven': 11, 'twelve': 12,
        'thirteen': 13, 'fourteen': 14, 'fifteen': 15, 'sixteen': 16, 'seventeen': 17,
        'eighteen': 18, 'nineteen': 19, 'twenty': 20
    }

    lowercase_num_str = num_str.lower()
    if lowercase_num_str in number_words:
        return True

    digits = [ str(x) for x in number_words.values() ]
    if lowercase_num_str in digits:
      return True

    return False

def filter_sublists(input_list):
    filtered_list = [[num for num in sublist if is_number_between_zero_and_twenty(num)] for sublist in input_list]
    return filtered_list

filtered_all_ranks = filter_sublists(all_ranks)

filtered_all_ranks[0]

"""### **Evaluation**"""

def eliminate_duplicates(rank):
    number_words = {
        'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6,
        'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10, 'eleven': 11, 'twelve': 12,
        'thirteen': 13, 'fourteen': 14, 'fifteen': 15, 'sixteen': 16, 'seventeen': 17,
        'eighteen': 18, 'nineteen': 19, 'twenty': 20
    }
    new_rank = []
    for item in rank:
      if item in number_words:
        new_rank.append(str(number_words[item]))
      else:
        new_rank.append(item)

    output_rank = []
    for item in new_rank:
      if item in output_rank:
        continue
      else:
        output_rank.append(item)

    return output_rank

def get_score(gold, prediction):
  denominator = np.absolute(int(gold) - int(prediction)) + 1
  return 1 / denominator

def get_qrels_dict(answer):
  output = {}
  for i in range(0, 21):
    output[f"d_{i}"] = get_score(answer, i)
  return output

def get_runs_dict(rank):
  output = {}
  for idx, item in enumerate(rank):
    output[f"d_{item}"] = 1 / ( idx + 1 )
  return output

answers = data['answer'].values.tolist()

qrels_dict = {}
runs_dict = {}

for idx_rank, rank in enumerate(filtered_all_ranks):
  rank_answer = answers[idx_rank]
  qrels_dict[f"q_{idx_rank}"] = get_qrels_dict(rank_answer)

  clean_rank = eliminate_duplicates(rank)
  runs_dict[f"q_{idx_rank}"] = get_runs_dict(clean_rank)

!pip install ranx -qqq

from ranx import Qrels, Run

qrels = Qrels(qrels_dict)
run = Run(runs_dict)

from ranx import evaluate

evaluate(qrels, run, ["map@5", "mrr"])

"""## Evaluation of entailmemt"""

data

# Assuming your data is in a DataFrame called 'df'
data['entailment'] = data['entailment'].str.replace('\d+', "[MASK]", regex=True)
# Print the modified DataFrame
data

# Prepare inputs for each image
encoded_images = []
for _, row in data.iterrows():
    # Get the image bytes for the current row
    image_bytes = row['image']['bytes']

    # Create a BytesIO object from the image bytes
    image_file = BytesIO(image_bytes)

    # Open the image using PIL
    image = Image.open(image_file)

    encoded_images.append(image)

text = data['entailment'].values.tolist()

assert len(text) == len(encoded_images)

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
model = ViltForMaskedLM.from_pretrained("dandelin/vilt-b32-mlm")

all_ranks = []

for text, image in zip(text, encoded_images):
  encoding = processor(images=image, text=text, return_tensors="pt")

  outputs = model(**encoding)
  inferred_token = [text]

# gradually fill in the MASK tokens, one by one

  mask_outputs = []
  with torch.no_grad():

      encoded = processor.tokenizer(inferred_token, truncation=True, padding=True)

      input_ids = torch.tensor(encoded.input_ids)

      encoded = encoded["input_ids"][0][1:-1]

      outputs = model(input_ids=input_ids, pixel_values=encoding.pixel_values)

      mlm_logits = outputs.logits[0]  # shape (seq_len, vocab_size)

      # only take into account text features (minus CLS and SEP token)

      mlm_logits = mlm_logits[1:input_ids.shape[1] - 1, :]

      # Calculate softmax once outside the loop
      mlm_softmax = mlm_logits.softmax(dim=-1).topk(mlm_logits.shape[1], dim=-1)

      mlm_topk_values, mlm_topk_ids = mlm_softmax
      for i in range(0, mlm_logits.shape[1]):
          mlm_values = mlm_topk_values[:, i]
          mlm_ids = mlm_topk_ids[:, i]
          mlm_values[torch.tensor(encoded) != 103] = 0
          select = mlm_values.argmax().item()
          select_id = mlm_ids[select].item()
          output = processor.decode([select_id])
          mask_outputs.append(output)

  all_ranks.append(mask_outputs)

filtered_all_ranks = filter_sublists(all_ranks)

answers = data['answer'].values.tolist()

qrels_dict = {}
runs_dict = {}

for idx_rank, rank in enumerate(filtered_all_ranks):
  rank_answer = answers[idx_rank]
  qrels_dict[f"q_{idx_rank}"] = get_qrels_dict(rank_answer)

  clean_rank = eliminate_duplicates(rank)
  runs_dict[f"q_{idx_rank}"] = get_runs_dict(clean_rank)

from ranx import Qrels, Run

qrels = Qrels(qrels_dict)
run = Run(runs_dict)

from ranx import evaluate

evaluate(qrels, run, ["map@5", "mrr"])

